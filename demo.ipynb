{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully unsupervised dynamic MRI reconstruction via geometrotemporal equivariance\n",
    "\n",
    "Paper | [Repo](https://github.com/Andrewwango/ddei) | [Website](https://andrewwango.github.io/ddei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/demo_results.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim**: reconstruct dynamic MRI videos from accelerated undersampled measurements $\\mathbf{y}=\\mathbf{Ax}$ where $\\mathbf{A}$ is an undersampled Fourier operator.\n",
    "\n",
    "**Applications**: real-time cardiac imaging, free-breathing motion, vocal tract speech...\n",
    "\n",
    "**Goals**:\n",
    "\n",
    "- Capture true motion: aperiodicities, irregularities: real-time MRI\n",
    "- Capture higher spatiotemporal resolution with fewer measurements (leading to faster, cheaper, portable MRI)\n",
    "\n",
    "**Why is it hard?** ground truth is impossible to truly obtain! There is no such thing as true fully-sampled dynamic MRI data at the same frame rate as the measurement data. Hence all supervised methods ([CineNet](https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.14809), [transformers](https://link.springer.com/chapter/10.1007/978-3-031-52448-6_39)...) are fundamentally flawed - an implicit [_data crime_](https://www.pnas.org/doi/full/10.1073/pnas.2117203119). The best pseudo-ground-truth, e.g. gating/cine imaging, must assume periodicity and all methods that use this cannot capture true motion and its irregularities - which is often of interest in medical imaging. Therefore we need unsupervised methods.\n",
    "\n",
    "**Our method** we posit that the unknown set $\\mathcal{X}$ of MRI videos is $G$-invariant: $\\forall x\\in\\mathcal{X},g\\cdot x\\in\\mathcal{X}\\forall g\\in G$. We propose a spatiotemporal group combining temporal invariance and diffeomorphic invariance: see our paper for more details. We use the [Equivariant Imaging](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf) framework to constrain this with a ground-truth-free loss, which we call **D**ynamic **D**iffeomorphic **E**quivariant **I**maging (**DDEI**).\n",
    "\n",
    "**Results**: see [below](#full-results).\n",
    "\n",
    "You can easily implement our method using the [`deepinv`](https://deepinv.github.io) library. See [train.py](train.py) for a full training demo including training and evaluating competitors. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import deepinv as dinv\n",
    "\n",
    "from utils import Trainer, ArtifactRemovalCRNN, CRNN, DeepinvSliceDataset, CineNetDataTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dynamic MRI physics:\n",
    "\n",
    "Define accelerated dynamic MRI. We set the (4x, 8x, 16x) undersampling mask on-the-fly as it varies per subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics = dinv.physics.DynamicMRI(img_size=(1, 2, 12, 512, 256), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DDEI loss function:\n",
    "\n",
    "See [train.py](train.py) for full demo of how to train with competitors' losses using [`deepinv`](https://deepinv.github.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform = dinv.transform.ShiftTime() | (dinv.transform.CPABDiffeomorphism() | dinv.transform.Rotate())\n",
    "loss = [dinv.loss.MCLoss(), dinv.loss.EILoss(transform=transform)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data:\n",
    "\n",
    "We use a real-world cardiac 2D+t dataset from the [2023 CMRxRecon challenge](https://cmrxrecon.github.io/). This dataset has fully-sampled cine measurements which we use as GT for evaluation, but we emphasise our method does not require gated GT for training. We then simulate 2D+t random Cartesian undersampling masks. For more details on how to get started see [train.py](train.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset cache file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = DeepinvSliceDataset(\n",
    "    root=\"data/CMRxRecon\",\n",
    "    transform=CineNetDataTransform(time_window=12, apply_mask=True, normalize=True), \n",
    "    set_name=\"TrainingSet\",\n",
    "    acc_folders=[\"FullSample\"],\n",
    "    mask_folder=\"TimeVaryingGaussianMask08\",\n",
    "    dataset_cache_file=\"dataset_cache_new.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neural network:\n",
    "\n",
    "For $f_\\theta$ we use a very small [CRNN](https://ieeexplore.ieee.org/document/8425639), a lightweight unrolled network with 2 unrolled iterations and 1154 parameters. Our framework is **NN-agnostic** and any state-of-the-art NN can be used as the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ArtifactRemovalCRNN(CRNN(num_cascades=2)).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network!\n",
    "\n",
    "We train the network using a modified [`deepinv.Trainer`](https://deepinv.github.io/deepinv/stubs/deepinv.Trainer.html). For full training demo, see [train.py](train.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    physics = physics,\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    train_dataloader = DataLoader(dataset=dataset),\n",
    "    losses = loss,\n",
    "    metrics = dinv.metric.PSNR(complex_abs=True, max_pixel=None)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full results\n",
    "\n",
    "Test set example cardiac long axis views (above 2 rows) and short axis slice (below) reconstruction results:\n",
    "\n",
    "![](img/results_fig_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
